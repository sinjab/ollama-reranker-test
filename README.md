# Ollama Reranker Testing Suite

A comprehensive testing framework for BGE and Qwen reranker models. This project validates and benchmarks different reranker implementations for document ranking and relevance scoring with **100% success rate** across all models.

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- [uv](https://docs.astral.sh/uv/) package manager
- Ollama with reranking support

### Installation & Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/sinjab/ollama-reranker-test.git
   cd ollama-reranker-test
   ```

2. **Set up reranker models** (REQUIRED):
   ğŸ“– **See [MODEL_SETUP.md](MODEL_SETUP.md) for complete model installation guide**
   
   Quick setup:
   ```bash
   # Download GGUF models from Hugging Face (see MODEL_SETUP.md for links)
   mkdir -p models
   # Download .gguf files to models/ directory
   
   # Create BGE models (Production Ready âœ…)
   ollama create bge-base -f templates/Modelfile.bge-base
   ollama create bge-large -f templates/Modelfile.bge-large
   ollama create bge-v2-m3 -f templates/Modelfile.bge-v2-m3
   
   # Create Qwen3 models (Production Ready âœ…)
   ollama create qwen3-0.6b -f templates/Modelfile.qwen3-0.6b
   ollama create qwen3-4b -f templates/Modelfile.qwen3-4b
   ollama create qwen3-8b -f templates/Modelfile.qwen3-8b
   
   # Or use automation: ./setup_models.sh
   ```

3. **Install dependencies**:
   ```bash
   uv sync
   ```

4. **Run the test suite**:
   ```bash
   # Test all models and implementations
   uv run python test_reranker.py
   
   # Test specific model type
   uv run python test_reranker.py --model-type bge
   uv run python test_reranker.py --model-type qwen
   
   # Test specific implementation
   uv run python test_reranker.py --implementation official
   uv run python test_reranker.py --implementation ollama
   ```

## ğŸ¯ Current Status

### âœ… Production Ready Models (100% Success Rate)
- **BGE Models**: All 3 variants (`bge-base`, `bge-large`, `bge-v2-m3`) with excellent score differentiation
- **Qwen Models**: All 3 variants (`qwen3-0.6b`, `qwen3-4b`, `qwen3-8b`) with perfect ranking accuracy
- **Performance**: 0.073-0.324s response times, perfect ranking accuracy
- **Status**: All models ready for production RAG applications

### ğŸ”§ Critical Technical Fix
**BGE Models Fixed**: All BGE models now work perfectly with Ollama after adding the TEMPLATE directive:

```
TEMPLATE """Query: {{ .Query }}
Document: {{ .Document }}
Relevance:"""
```

This template ensures proper formatting for cross-encoder models in Ollama's reranking API.

### ğŸ“ˆ Test Results
- **Overall Success Rate**: 100% (72/72 tests pass)
- **All Models**: Perfect score differentiation and ranking accuracy
- **Performance**: 3-10x faster with Ollama implementations

## ğŸ“Š Supported Models

### BGE Rerankers (Cross-Encoder Models)
| Model | Performance | Use Case | Quantization |
|-------|-------------|----------|--------------|
| `BAAI/bge-reranker-v2-m3` | High | Production workloads | Q4_K_M GGUF |
| `BAAI/bge-reranker-base` | Balanced | General purpose | Q4_K_M GGUF |
| `BAAI/bge-reranker-large` | Maximum | High accuracy needs | Q4_K_M GGUF |

### Qwen Rerankers (Instruction-Based Models)
| Model | Performance | Use Case | Quantization |
|-------|-------------|----------|--------------|
| `Qwen/Qwen3-Reranker-0.6B` | Fast | Lightweight reasoning | Q4_K_M GGUF |
| `Qwen/Qwen3-Reranker-4B` | High | Advanced reasoning | Q4_K_M GGUF |
| `Qwen/Qwen3-Reranker-8B` | Maximum | Best accuracy | Q4_K_M GGUF |

> **ğŸ“¦ Model Versions**: This testing framework uses Q4_K_M quantized GGUF versions of all models for optimal performance and reduced memory usage. The Ollama implementations utilize these quantized models:
> - `bge-reranker-base-q4_k_m.gguf`
> - `bge-reranker-large-q4_k_m.gguf` 
> - `bge-reranker-v2-m3-Q4_K_M.gguf`
> - `Qwen3-Reranker-0.6B.Q4_K_M.gguf`
> - `Qwen3-Reranker-4B.Q4_K_M.gguf`
> - `Qwen3-Reranker-8B.Q4_K_M.gguf`

## ğŸ§ª Test Cases

The framework includes comprehensive test cases covering:

- **Basic Functionality**: Simple query-document ranking
- **Domain-Specific**: Machine learning, cooking, geography
- **Edge Cases**: Empty documents, invalid data, single documents
- **Performance**: Execution time and memory usage tracking

### Test Case Format

```json
{
  "query": "What is machine learning?",
  "documents": [
    "Machine learning is a subset of artificial intelligence.",
    "Cooking is the art of preparing food.",
    "Deep learning uses neural networks."
  ],
  "top_n": 2,
  "instruction": "Find AI topics"
}
```

## ğŸ“ˆ Sample Results

```
ğŸ¤– UNIFIED RERANKER TEST FRAMEWORK
==================================================

ğŸ”§ Testing BGE OFFICIAL: BAAI/bge-reranker-v2-m3
============================================================

ğŸ“‹ Testing: test_capital
Query: What is the capital of China?
Documents: 3
âœ… SUCCESS (0.250s)
ğŸ“ˆ Rankings:
  1. Beijing is the capital of China... (score: 1.0000)
  2. China is a large country in Asia... (score: 0.0745)
  3. Paris is the capital of France... (score: 0.0003)

ğŸ“Š TEST SUMMARY
==================================================
ğŸ“Š BGE Models: 6/6 (100% success)
ğŸ“Š Qwen Models: 6/6 (100% success)
ğŸ“Š Ollama API: 6/6 (100% success)
ğŸ“Š Official Implementation: 6/6 (100% success)

ğŸ“Š OVERALL SUMMARY
Total Tests: 72 (12 models Ã— 6 test cases)
Successful Tests: 72
Success Rate: 100.0%
âœ… All models demonstrate perfect ranking consistency
```

## ğŸ“ Project Structure

```
ollama-reranker-test/
â”œâ”€â”€ test_reranker.py          # Unified test framework
â”œâ”€â”€ compare_results.py        # Results comparison tool
â”œâ”€â”€ MODEL_SETUP.md           # Complete model installation guide
â”œâ”€â”€ setup_models.sh          # Automated Ollama model creation
â”œâ”€â”€ validate_models.sh       # Quick model validation script
â”œâ”€â”€ models/                  # GGUF model files (download from HF links)
â”‚   â”œâ”€â”€ bge-reranker-base-Q4_K_M.gguf
â”‚   â”œâ”€â”€ bge-reranker-large-Q4_K_M.gguf
â”‚   â”œâ”€â”€ bge-reranker-v2-m3-Q4_K_M.gguf
â”‚   â”œâ”€â”€ Qwen3-Reranker-0.6B.Q4_K_M.gguf
â”‚   â”œâ”€â”€ Qwen3-Reranker-4B.Q4_K_M.gguf
â”‚   â””â”€â”€ Qwen3-Reranker-8B.Q4_K_M.gguf
â”œâ”€â”€ templates/               # Ollama Modelfile templates
â”‚   â”œâ”€â”€ Modelfile.bge-base
â”‚   â”œâ”€â”€ Modelfile.bge-large
â”‚   â”œâ”€â”€ Modelfile.bge-v2-m3
â”‚   â”œâ”€â”€ Modelfile.qwen3-0.6b
â”‚   â”œâ”€â”€ Modelfile.qwen3-4b
â”‚   â””â”€â”€ Modelfile.qwen3-8b
â”œâ”€â”€ tests/                   # Test case definitions
â”‚   â”œâ”€â”€ test_basic.json
â”‚   â”œâ”€â”€ test_capital.json
â”‚   â”œâ”€â”€ test_cooking.json
â”‚   â”œâ”€â”€ test_empty.json
â”‚   â”œâ”€â”€ test_invalid.json
â”‚   â”œâ”€â”€ test_ml.json
â”‚   â””â”€â”€ test_simple.json
â”œâ”€â”€ results/                 # Generated test results
â”œâ”€â”€ pyproject.toml          # Project configuration
â””â”€â”€ LICENSE                 # MIT License
```

## ğŸ¯ Testing Results & Insights

### Model Comparison Results
- **12 total models tested**: 6 BGE + 6 Qwen variants
- **Perfect correlation (1.0000)** between all model implementations
- **100% success rate** across all 72 test combinations
- **Consistent ranking accuracy** regardless of implementation type

### Performance Categories

#### BGE Models Performance
| Model | Implementation | Avg Time (s) | Sample Score |
|-------|----------------|--------------|--------------|
| bge-large | Ollama | 0.073 | 0.9995 |
| bge-v2-m3 | Ollama | 0.073 | 0.9995 |
| bge-base | Ollama | 0.081 | 0.9995 |
| bge-reranker-base | Official | 0.120 | 0.9994 |
| bge-reranker-large | Official | 0.206 | 0.9997 |
| bge-reranker-v2-m3 | Official | 0.250 | 1.0000 |

#### Qwen Models Performance  
| Model | Implementation | Avg Time (s) | Sample Score |
|-------|----------------|--------------|--------------|
| qwen3-0.6b | Ollama | 0.075 | 0.9995 |
| Qwen3-Reranker-0.6B | Official | 0.258 | 0.9995 |
| qwen3-4b | Ollama | 0.199 | 0.9995 |
| Qwen3-Reranker-4B | Official | 1.073 | 0.9982 |
| qwen3-8b | Ollama | 0.324 | 0.9995 |
| Qwen3-Reranker-8B | Official | 3.129 | 0.9945 |

### Recommendations

#### Production Use
- **bge-large (Ollama)** - Best performance/accuracy balance (3x faster than official)
- **bge-v2-m3 (Ollama)** - High-performance alternative (3x faster than official)
- **qwen3-4b (Ollama)** - For reasoning-heavy tasks (5x faster than official)

#### Development & Testing
- **Ollama implementations** - Faster iteration cycles
- **Official implementations** - Maximum accuracy validation
- **All models** - Perfect reliability with 100% success rate

## âš™ï¸ Configuration

### Model Settings
- **Quantization**: Q4_K_M GGUF format for 4-bit quantization with mixed precision
- **Memory Optimization**: Quantized models reduce VRAM usage by ~75% 
- **Performance**: Maintains accuracy while significantly improving inference speed
- **Normalization**: `normalize=True` for 0-1 confidence scores
- **FP16**: `use_fp16=True` for performance optimization on compatible hardware
- **Caching**: Models are cached to avoid repeated downloads

### Critical BGE Configuration
BGE models require the TEMPLATE directive for proper Ollama integration:

```dockerfile
# BGE Modelfile template (required for cross-encoder models)
FROM ./models/bge-reranker-v2-m3-Q4_K_M.gguf
TEMPLATE """Query: {{ .Query }}
Document: {{ .Document }}
Relevance:"""
PARAMETER stop "</s>"
PARAMETER temperature 0.0
```

### Adding New Models

Update the `MODEL_CONFIGS` dictionary in `test_reranker.py`:

```python
MODEL_CONFIGS = {
    'new_model_type': {
        'official': {
            'models': ["path/to/model"],
            'default': "path/to/model"
        },
        'ollama': {
            'models': ["ollama_model_name"],
            'default': "ollama_model_name"
        }
    }
}
```

## ğŸ“Š Performance Benchmarks

Based on comprehensive testing across all models using **Q4_K_M quantized GGUF versions**:

### Speed Rankings (Fastest to Slowest)

| Rank | Model | Implementation | Avg Time (s) | Use Case |
|------|-------|----------------|--------------|----------|
| 1 | bge-large | Ollama | 0.073s | **Production recommended** |
| 2 | bge-v2-m3 | Ollama | 0.073s | High performance |
| 3 | bge-base | Ollama | 0.081s | Balanced option |
| 4 | qwen3-0.6b | Ollama | 0.075s | Fast reasoning |
| 5 | bge-reranker-base | Official | 0.120s | Validation |
| 6 | bge-reranker-large | Official | 0.206s | Accuracy validation |
| 7 | Qwen3-Reranker-0.6B | Official | 0.258s | Lightweight reasoning |
| 8 | bge-reranker-v2-m3 | Official | 0.250s | Maximum accuracy |
| 9 | qwen3-4b | Ollama | 0.199s | Advanced reasoning |
| 10 | qwen3-8b | Ollama | 0.324s | Best reasoning |
| 11 | Qwen3-Reranker-4B | Official | 1.073s | Complex reasoning |
| 12 | Qwen3-Reranker-8B | Official | 3.129s | Maximum accuracy |

### Key Performance Insights
- **Ollama implementations are 3-10x faster** than official implementations
- **BGE models significantly outperform Qwen models** in speed
- **All models achieve 100% success rate** with perfect ranking consistency
- **bge-large (Ollama)** offers the best performance/accuracy balance

## ğŸ¯ Key Achievements

- âœ… **12/12 models working** (100% success rate)
- âœ… **3-10x performance improvements** with Ollama
- âœ… **Perfect accuracy** maintained across all models
- âœ… **Comprehensive test coverage** (6 test cases)
- âœ… **Production-ready** configurations for all models
- âœ… **BGE models fixed** with TEMPLATE directive
- âœ… **Qwen models work natively** with Ollama

## ğŸ› Troubleshooting

### Common Issues

1. **Model Download Failures**:
   ```bash
   # Clear cache and retry
   rm -rf ~/.cache/huggingface/hub/
   uv run python test_reranker.py
   ```

2. **BGE Models Not Working**:
   ```bash
   # Ensure TEMPLATE directive is present in Modelfile
   cat templates/Modelfile.bge-base
   
   # Should include:
   # TEMPLATE """Query: {{ .Query }}
   # Document: {{ .Document }}
   # Relevance:"""
   ```

3. **Memory Issues**:
   - Use smaller models (base instead of large)
   - Monitor system memory usage

4. **Dependency Conflicts**:
   ```bash
   # Recreate environment
   rm -rf .venv
   uv sync
   ```

### Error Messages

- `ModuleNotFoundError`: Run `uv sync` to install dependencies
- `CUDA out of memory`: Use CPU-only mode or smaller models
- API connection errors: Ensure Ollama server is running for API tests

## ğŸ¤ Contributing

1. **Add Test Cases**: Create new JSON files in `tests/` directory
2. **Improve Error Handling**: Add robust error recovery
3. **Performance Optimization**: Optimize model loading and scoring
4. **Documentation**: Update README and add inline comments

## ğŸ“š API Reference

### Output Format

```python
{
    "success": True,
    "results": [
        {
            "index": 0,
            "document": "Document text...",
            "relevance_score": 0.9994,
            "raw_response": "0.9994"
        }
    ],
    "time": 0.074,
    "error": None
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- [BAAI](https://github.com/FlagOpen/FlagEmbedding) for the BGE reranker models
- [Qwen Team](https://github.com/QwenLM/Qwen) for the Qwen reranker models
- [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding) for the Python library
- [uv](https://docs.astral.sh/uv/) for fast Python package management
